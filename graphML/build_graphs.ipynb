{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent App with Google Generative AI and Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import re \n",
    "import os\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "import requests\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from helpers.df_helpers import documents2Dataframe\n",
    "from helpers.clean_data import clean_text\n",
    "import gcloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project_id= \"elevated-module-440602-p8\"\n",
    "location='us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001DE10ECCDF0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001DE0F6BFF10>, root_client=<openai.lib.azure.AzureOpenAI object at 0x000001DE0F6D94F0>, root_async_client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001DE10ECCE20>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), disabled_params={'parallel_tool_calls': None}, azure_endpoint='https://easttest123.openai.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2023-03-15-preview', deployment_name='gpt-35-turbo', openai_api_version='2024-02-15-preview', openai_api_type='azure')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vertexai\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "vertexai.init(project=project_id,location=location)\n",
    "graph=Neo4jGraph(url=os.getenv('NEO4J_URI_ONLINE'),\n",
    "                 username=os.getenv('NEO4J_USERNAME_ONLINE'),\n",
    "                 password=os.getenv('NEO4J_PASSWORD_ONLINE')\n",
    "                #  database=os.getenv('NEO4J_DATABASE')\n",
    ")\n",
    "\n",
    "llm=AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv('AZURE_OPENAI_DEPLOYMENT_MODEL'),\n",
    "    api_version=os.getenv('AZURE_OpenAI_API_VERSION'),\n",
    "    temperature=0,\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/chat-bison-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='PaLM 2 Chat (Legacy)',\n",
       "       description='A legacy text-only model optimized for chat conversations',\n",
       "       input_token_limit=4096,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
       "       temperature=0.25,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/text-bison-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='PaLM 2 (Legacy)',\n",
       "       description='A legacy model that understands text and generates text as an output',\n",
       "       input_token_limit=8196,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
       "       temperature=0.7,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/embedding-gecko-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding Gecko',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=1024,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Latest',\n",
       "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
       "                    'model.'),\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro',\n",
       "       description='The best model for scaling across a wide range of tasks',\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro',\n",
       "       description='The best model for scaling across a wide range of tasks',\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
       "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
       "                    'model that supports tuning.'),\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-vision-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Vision',\n",
       "       description='The best image understanding model to handle a broad range of applications',\n",
       "       input_token_limit=12288,\n",
       "       output_token_limit=4096,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.4,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=32),\n",
       " Model(name='models/gemini-pro-vision',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Vision',\n",
       "       description='The best image understanding model to handle a broad range of applications',\n",
       "       input_token_limit=12288,\n",
       "       output_token_limit=4096,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.4,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=32),\n",
       " Model(name='models/gemini-1.5-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro Latest',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-pro-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro 001',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-pro-002',\n",
       "       base_model_id='',\n",
       "       version='002',\n",
       "       display_name='Gemini 1.5 Pro 002',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-pro-exp-0801',\n",
       "       base_model_id='',\n",
       "       version='exp-0801',\n",
       "       display_name='Gemini 1.5 Pro Experimental 0801',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-pro-exp-0827',\n",
       "       base_model_id='',\n",
       "       version='exp-0827',\n",
       "       display_name='Gemini 1.5 Pro Experimental 0827',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash Latest',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash 001',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash-001-tuning',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=16384,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-exp-0827',\n",
       "       base_model_id='',\n",
       "       version='exp-0827',\n",
       "       display_name='Gemini 1.5 Flash Experimental 0827',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash-002',\n",
       "       base_model_id='',\n",
       "       version='002',\n",
       "       display_name='Gemini 1.5 Flash 002',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B 001',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B Latest',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/text-embedding-004',\n",
       "       base_model_id='',\n",
       "       version='004',\n",
       "       display_name='Text Embedding 004',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/aqa',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Model that performs Attributed Question Answering.',\n",
       "       description=('Model trained to return answers to questions that are grounded in provided '\n",
       "                    'sources, along with estimating answerable probability.'),\n",
       "       input_token_limit=7168,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateAnswer'],\n",
       "       temperature=0.2,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=40)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt definition\n",
    "import google.generativeai as palm\n",
    "import os\n",
    "palm.configure(api_key=os.getenv('GOOGLE_AI_API_KEY'))\n",
    "\n",
    "model_list=[_ for _ in palm.list_models()]\n",
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Satoshi Nakamoto WikipediaJump to contentMain menuMain menumove to sidebarhideNavigationMain pageContentsCurrent eventsRandom articleAbout WikipediaContact usContributeHelpLearn to editCommunity portalRecent changesUpload fileSearchSearchAppearanceDonateCreate accountLog inPersonal toolsDonate Create account Log inPages for logged out editors learn moreContributionsTalkContentsmove to sidebarhideTop1Development of bitcoin2Characteristics and identityToggle Characteristics and identity subsection21Possible identities211Hal Finney212Dorian Nakamoto213Nick Szabo214Craig Wright215Other candidates3In popular culture4References5External linksToggle the table of contentsSatoshi Nakamoto52 languagesAfrikaansAzrbaycanca BnlmgCataletinaDanskDeutschEestiEspaolEsperantoFranaisHausaBahasa IndonesiaItalianoKiswahiliKurdLatvieuLombardMagyar NederlandsNorsk bokmlOccitanOzbekcha PolskiPortugusRomnShqipSimple EnglishSloveninaSuomiTagalogTrke UyghurcheTing VitEdit linksArticleTalkEnglishReadView sourceView historyToolsToolsmove to sidebarhideActionsReadView sourceView historyGeneralWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codePrintexportDownload as PDFPrintable versionIn other projectsWikimedia CommonsWikiquoteWikidata itemAppearancemove to sidebarhideFrom Wikipedia the free encyclopediaPseudonym of the designer and developer of BitcoinSatoshi NakamotoA statue in Budapest dedicated to Satoshi NakamotoKnownforInventing bitcoin implementing the first blockchainScientific careerFieldsDigital currencies computer science cryptographySatoshi Nakamoto is the name used by the presumed pseudonymous1234 person or persons who developed bitcoin authored the bitcoin white paper and created and deployed bitcoins original reference implementation5 As part of the implementation Nakamoto also devised the first blockchain database6 Nakamoto was active in the development of bitcoin until December 20107There has been widespread speculation about Nakamotos true identity with various people posited as the person or persons behind the name Though Nakamotos name is Japanese and inscribed as a man living in Japan8 most of the speculation has involved software and cryptography experts in the United States or EuropeDevelopment of bitcoinSatoshi Nakamoto message embedded in the coinbase of the first blockNakamoto said that the work of writing bitcoins code began in the second quarter of 20079 On 18 August 2008 he or a colleague registered the domain name bitcoinorg10 and created a web site at that address On 31 October Nakamoto published a white paper on the cryptography mailing list at metzdowdcom describing a digital cryptocurrency titled Bitcoin A PeertoPeer Electronic Cash System111213On 9 January 2009 Nakamoto released version 01 of the bitcoin software on SourceForge and launched the network by defining the genesis block of bitcoin block number 0 which had a reward of 50 bitcoins1415716 Embedded in the'),\n",
       " Document(metadata={}, page_content='cryptocurrency titled Bitcoin A PeertoPeer Electronic Cash System111213On 9 January 2009 Nakamoto released version 01 of the bitcoin software on SourceForge and launched the network by defining the genesis block of bitcoin block number 0 which had a reward of 50 bitcoins1415716 Embedded in the coinbase transaction of this block is the text The Times 03Jan2009 Chancellor on brink of second bailout for banks17 citing a headline in the UK newspaper The Times published on that date18 This note has been interpreted as both a timestamp and a derisive comment on the alleged instability caused by fractionalreserve banking1918Nakamoto continued to collaborate with other developers on bitcoins software until mid2010 making all modifications to the source code himself He then gave control of the source code repository and network alert key to Gavin Andresen20 and transferred several related domains to various prominent members of the bitcoin community As of 2021 Nakamoto is estimated to own between 750000 and 1100000 bitcoin In November 2021 when bitcoin reached a value of over 68000 his net worth would have been up to 73 billion making him the 15thrichest person in the world at the time21Characteristics and identityNakamoto has never revealed personal information when discussing technical matters7 but has at times commented on banking and fractionalreserve banking Some speculated he was unlikely to be Japanese due to his nativelevel use of English7On his P2P Foundation profile as of 2012 Nakamoto claimed to be a 37yearold man who lived in Japan8 he cited his date of birth as 5 April 197522 Some theorize that the date referenced the signing of Executive Order 6102 which prohibited the ownership of gold coins in the United States and 1975 as the year it was repealed Author Dominic Frisby categorized the date as an obscure but brilliant reference and as extremely political22Some have considered that Nakamoto might be a team of people Dan Kaminsky a security researcher who read bitcoins code23 said that Nakamoto was either a team of people or a genius24 Laszlo Hanyecz a developer who had emailed Nakamoto had the feeling the code was too welldesigned for one person7 Andresen has said of Nakamotos code He was a brilliant coder but it was quirky25The use of British English in both source code comments and forum postings such as the expression bloody hard terms such as flat and maths and the spellings grey and colour17 led to speculation that Nakamoto or at least one person in a consortium claiming to be him was of Commonwealth origin71124 The reference to Londons Times newspaper in the first bitcoin block suggested to some a particular interest in the British government1726Stefan Thomas a Swiss software engineer and active community member graphed the timestamps of each of Nakamotos bitcoin forum posts more than 500 the chart showed a steep decline to almost none between 5 am and 11 am Greenwich Mean Time midnight to 6 am Eastern Standard Time This was between 2'),\n",
       " Document(metadata={}, page_content='Thomas a Swiss software engineer and active community member graphed the timestamps of each of Nakamotos bitcoin forum posts more than 500 the chart showed a steep decline to almost none between 5 am and 11 am Greenwich Mean Time midnight to 6 am Eastern Standard Time This was between 2 pm and 8 pm Japan Standard Time suggesting an unusual sleep pattern for someone living in Japan As this pattern held even on Saturdays and Sundays it suggested that Nakamoto was consistently asleep at this time7Possible identitiesNakamotos identity is unknown27 but speculations have focussed on various cryptography and computer science experts most of nonJapanese descent7 Bitcoiners and cryptographers have suggested various methods by which a person could prove their identity as Nakamoto such as moving the earliest bitcoins mined or signing a message with the key associated with the first bitcoins28 On the other hand a denial of being Nakamoto is very difficult to confirmHal FinneyHal Finney 4 May 1956 28 August 2014 was a prebitcoin cryptographic pioneer and the first person other than Nakamoto himself to use the software file bug reports and make improvements29 He also lived a few blocks from a man named Dorian Satoshi Nakamoto according to Forbes journalist Andy Greenberg30 Greenberg asked the writing analysis consultancy Juola Associates to compare a sample of Finneys writing to Nakamotos and found it to be the closest resemblance they had yet come across including when compared to candidates suggested by Newsweek Fast Company The New Yorker Ted Nelson and Skye Grey30 Greenberg theorized that Finney may have been a ghostwriter on Nakamotos behalf or that he simply used his neighbors identity as a drop or patsy whose personal information is used to hide online exploits but after meeting Finney seeing the emails between him and Nakamoto and his bitcoin wallets history including the first transaction from Nakamoto to him which he forgot to pay back and hearing his denial Greenberg concluded that Finney was telling the truth Juola Associates also found that Nakamotos emails to Finney more closely resemble Nakamotos other writings than Finneys do Finneys fellow extropian and sometime coblogger Robin Hanson assigned a subjective probability of at least 15 that Hal was more involved than hes said before further evidence suggested that was not the case31Dorian NakamotoIn a highprofile March 2014 article in Newsweek32 journalist Leah McGrath Goodman identified Dorian Prentice Satoshi Nakamoto a JapaneseAmerican man living in California whose birth name is Satoshi Nakamoto323334 as the Nakamoto in question Besides his name Goodman pointed to a number of facts that circumstantially suggested he was the bitcoin inventor32 Trained as a physicist at California State Polytechnic University Pomona Nakamoto worked as a systems engineer on classified defense projects and computer engineer for technology and financial information services companies According to his daughter'),\n",
       " Document(metadata={}, page_content='suggested he was the bitcoin inventor32 Trained as a physicist at California State Polytechnic University Pomona Nakamoto worked as a systems engineer on classified defense projects and computer engineer for technology and financial information services companies According to his daughter Nakamoto was laid off twice in the early 1990s turned libertarian and encouraged her to start her own business not under the governments thumb The articles seemingly biggest piece of evidence was that when Goodman asked him about bitcoin during a brief inperson interview Nakamoto seemed to confirm his identity as its founder saying I am no longer involved in that and I cannot discuss it Its been turned over to other people They are in charge of it now I no longer have any connection3235The articles publication led to a flurry of media interest including reporters camping out near Nakamotos house and chasing him by car when he drove to an interview36 Later that day the pseudonymous Nakamotos P2P Foundation account posted its first message in five years I am not Dorian Nakamoto3738 In a subsequent interview Nakamoto denied all connection to bitcoin saying he had never heard of it before and that he had misinterpreted Goodmans question as about his previous work for military contractors much of which was classified39 In a Reddit askmeanything interview he said he had misinterpreted Goodmans question as related to his work for Citibank40 In September the P2P Foundation account posted another message saying it had been hacked raising questions over the authenticity of the message six months earlier4142Nick SzaboIn December 2013 blogger Skye Grey linked Nick Szabo to the bitcoin white paper using stylometric analysis434445 Szabo is a decentralized currency enthusiast and published a paper on bit gold one of bitcoins precursors He is known to have been interested in using pseudonyms in the 1990s46 In a May 2011 article Szabo said of bitcoins creator Myself Wei Dai and Hal Finney were the only people I know of who liked the idea or in Dais case his related idea enough to pursue it to any significant extent until Nakamoto assuming Nakamoto is not really Finney or Dai47Financial author Dominic Frisby provides much circumstantial evidence but as he admits no proof that Nakamoto is Szabo48 Szabo has denied being Nakamoto In a July 2014 email to Frisby he wrote Thanks for letting me know Im afraid you got it wrong doxing me as Satoshi but Im used to it49 Nathaniel Popper wrote in The New York Times that the most convincing evidence pointed to a reclusive American man of Hungarian descent named Nick Szabo50Craig WrightSee also Craig Steven Wright BitcoinOn 8 December 2015 Wired wrote that Craig Steven Wright an Australian academic either invented bitcoin or is a brilliant hoaxer who very badly wants us to believe he did51 Wright took down his Twitter account and neither he nor his exwife responded to press inquiries The same day Gizmodo published a story with evidence')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_input=(\"https://en.wikipedia.org/wiki/Satoshi_Nakamoto\")\n",
    "loader=WebBaseLoader([url_input])\n",
    "data=loader.load().pop().page_content\n",
    "from helpers.clean_data import clean_text\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "data=clean_text(data)\n",
    "documents=[Document(page_content=data)]\n",
    "\n",
    "# split the data into chunks \n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=3000,chunk_overlap=300)\n",
    "smaller_doc=splitter.split_documents(documents)\n",
    "smaller_doc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_entity_prompt_tpl=\"\"\"From the biography text of a famous person or celebrity below, extract Entities and Relationships strictly as instructed:\n",
    "1. Identify all significant entities and relationships from the text, categorizing each as described below. The `id` property of each entity must be unique and alphanumeric, to ensure distinct nodes in Neo4j. You will be using this `id` property to define relationships between entities. Only use the entity types below; do NOT create new types.\n",
    "\n",
    "   **Entity Types:**\n",
    "   - **Person**: label:'Person', id:string, role:string, description:string // Summary of the individual’s primary role or identity.\n",
    "   - **Achievement**: label:'Achievement', id:string, description:string // Major accomplishments, awards, notable works, or contributions.\n",
    "   - **Organization**: label:'Organization', id:string, role:string, description:string // Companies, institutions, or groups the person is associated with.\n",
    "   - **Event**: label:'Event', id:string, description:string // Important events related to the individual (e.g., awards, major milestones).\n",
    "   - **Relationship**: label:'Relationship', id:string, type:string, source:string, target:string // Defines the connection type (e.g., \"Collaborated with,\" \"Founded,\" \"Won\") and references `source` and `target` entities by `id`.\n",
    "\n",
    "2. **Description Property Requirements**:\n",
    "   - Each description should be a concise summary, no more than 100 characters, focused on the essential detail.\n",
    "   - **Person**: Summarize their primary identity (e.g., \"Renowned American actor\").\n",
    "   - **Achievement**: Specify the nature of the accomplishment (e.g., \"Oscar-winning performance in 'Good Will Hunting'\").\n",
    "   - **Organization**: Indicate the person’s role or connection (e.g., \"Co-founder of Tesla, Inc.\").\n",
    "   - **Event**: Highlight the key details of the event (e.g., \"Nobel Prize in Physics, 1921\").\n",
    "   - **Relationship**: Clearly describe the connection and indicate its direction, ensuring each relationship is accurately categorized.\n",
    "\n",
    "3. **Rules for Extraction**:\n",
    "   - Avoid fictional or inferred data; extract only verifiable entities and relationships.\n",
    "   - Do not create duplicate entities.\n",
    "   - Only extract entities directly relevant to the person’s role, achievements, or associations.\n",
    "   - Avoid personal anecdotes or unrelated information.\n",
    "\n",
    "4. **Example Output JSON**:\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\"label\":\"Person\",\"id\":\"person1\",\"role\":\"Actor\",\"description\":\"Renowned American film actor\"},\n",
    "    {\"label\":\"Achievement\",\"id\":\"achievement1\",\"description\":\"Oscar-winning performance in 'Good Will Hunting'\"},\n",
    "    {\"label\":\"Organization\",\"id\":\"organization1\",\"role\":\"Co-founder\",\"description\":\"Co-founder of Tesla, Inc.\"},\n",
    "    {\"label\":\"Event\",\"id\":\"event1\",\"description\":\"Nobel Prize in Physics, 1921\"},\n",
    "    {\"label\":\"Relationship\",\"id\":\"relationship1\",\"type\":\"Collaborated with\",\"source\":\"person1\",\"target\":\"person2\"}\n",
    "  ]\n",
    "}\n",
    "\n",
    "Question: Now, extract the entities and relationships for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "def run_text_model(\n",
    "    project_id:str,\n",
    "    model_name:str,\n",
    "    temperature:float,\n",
    "    max_decode_steps:int,\n",
    "    top_k: int,\n",
    "    top_p: float,\n",
    "    prompt: str,\n",
    "    location: str=location,\n",
    "    tuned_model_name:str=\" \",\n",
    "    ) :\n",
    "    vertexai.init(project=project_id,location=location)\n",
    "    model=TextGenerationModel.from_pretrained(model_name)\n",
    "    if tuned_model_name:\n",
    "        model=TextGenerationModel.from_pretrained(model_name)\n",
    "    response=model.predict(\n",
    "        prompt,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entries_relationships(prompt,tune_model_name):\n",
    "    try:\n",
    "        res=run_text_model(project_id,'text-bison@001',0,1024,0.8,40,prompt,location,tune_model_name)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"\"\" \n",
    "Elon Reeve Musk FRS (/ˈiːlɒn/; born June 28, 1971) is a businessman and investor known for his key roles in the space company SpaceX and the automotive company Tesla, Inc. Other involvements include ownership of X Corp., the company that operates the social media platform X (formerly known as Twitter), and his role in the founding of the Boring Company, xAI, Neuralink, and OpenAI. He is one of the wealthiest individuals in the world; as of August 2024 Forbes estimates his net worth to be US$247 billion.[3]\n",
    "\n",
    "Musk was born in Pretoria, South Africa, to Maye (née Haldeman), a model, and Errol Musk, a businessman and engineer. Musk briefly attended the University of Pretoria before immigrating to Canada at the age of 18, acquiring citizenship through his Canadian-born mother. Two years later he matriculated at Queen's University at Kingston in Canada. Musk later transferred to the University of Pennsylvania and received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University, but dropped out after two days and, with his brother Kimbal, co-founded the online city guide software company Zip2. The startup was acquired by Compaq for $307 million in 1999. That same year Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal. In 2002 Musk acquired US citizenship. That October eBay acquired PayPal for $1.5 billion. Using $100 million of the money he made from the sale of PayPal, Musk founded SpaceX, a spaceflight services company, in 2002.\n",
    "\n",
    "In 2004, Musk was an early investor who provided most of the initial financing in the electric-vehicle manufacturer Tesla Motors, Inc. (later Tesla, Inc.), assuming the position of the company's chairman. He later became the product architect and, in 2008, the CEO. In 2006, Musk helped create SolarCity, a solar energy company that was acquired by Tesla in 2016 and became Tesla Energy. In 2013, he proposed a hyperloop high-speed vactrain transportation system. In 2015, he co-founded OpenAI, a nonprofit artificial intelligence research company. The following year Musk co-founded Neuralink, a neurotechnology company developing brain–computer interfaces, and The Boring Company, a tunnel construction company. In 2018 the U.S. Securities and Exchange Commission (SEC) sued Musk, alleging that he had falsely announced that he had secured funding for a private takeover of Tesla. To settle the case Musk stepped down as the chairman of Tesla and paid a $20 million fine. In 2022, he acquired Twitter for $44 billion, merged the company into the newly-created X Corp. and rebranded the service as X the following year. In March 2023, Musk founded xAI, an artificial-intelligence company.\n",
    "\n",
    "Musk's actions and expressed views have made him a polarizing figure.[4] He has been criticized for making unscientific and misleading statements, including COVID-19 misinformation, promoting right-wing conspiracy theories, and \"endorsing an antisemitic theory\"; he later apologized for the last of these.[5][4][6] His ownership of Twitter has been controversial because of the layoffs of large numbers of employees, an increase in hate speech, misinformation and disinformation posts on the website, and changes to website features, including verification. Musk has been active in American politics as a vocal and financial supporter of Donald Trump, becoming Trump's second-largest individual donor in October 2024.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 Project `122385713024` is not allowed to use Publisher Model `projects/elevated-module-440602-p8/locations/us-central1/publishers/google/models/text-bison@001`\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from string import Template\n",
    "import json\n",
    "\n",
    "results={'entities':[],'relationships':[]}\n",
    "_prompt=Template(dynamic_entity_prompt_tpl).substitute(ctext=clean_text(data))\n",
    "extraction=extract_entries_relationships(_prompt,'')\n",
    "print(extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.auth.transport .requests import Request\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "keypath='D:\\pythonProjects\\graphRAGAgent\\service_account.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentionals=Credentials.from_service_account_file(\n",
    "    keypath\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=project_id,location=location,credentials=credentionals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel\n",
    "generation_model=TextGenerationModel.from_pretrained('text-bison@001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPrecondition",
     "evalue": "400 Project `122385713024` is not allowed to use Publisher Model `projects/elevated-module-440602-p8/locations/us-central1/publishers/google/models/text-bison@001`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda\\envs\\pytorch\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\pytorch\\lib\\site-packages\\grpc\\_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1175\u001b[0m (\n\u001b[0;32m   1176\u001b[0m     state,\n\u001b[0;32m   1177\u001b[0m     call,\n\u001b[0;32m   1178\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[0;32m   1179\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[0;32m   1180\u001b[0m )\n\u001b[1;32m-> 1181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\pytorch\\lib\\site-packages\\grpc\\_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.FAILED_PRECONDITION\n\tdetails = \"Project `122385713024` is not allowed to use Publisher Model `projects/elevated-module-440602-p8/locations/us-central1/publishers/google/models/text-bison@001`\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B2607:f8b0:4023:1009::5f%5D:443 {created_time:\"2024-11-04T02:11:25.6269397+00:00\", grpc_status:9, grpc_message:\"Project `122385713024` is not allowed to use Publisher Model `projects/elevated-module-440602-p8/locations/us-central1/publishers/google/models/text-bison@001`\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgeneration_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhat is AI?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\vertexai\\language_models\\_language_models.py:1417\u001b[0m, in \u001b[0;36m_TextGenerationModel.predict\u001b[1;34m(self, prompt, max_output_tokens, temperature, top_k, top_p, stop_sequences, candidate_count, grounding_source, logprobs, presence_penalty, frequency_penalty, logit_bias, seed)\u001b[0m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Gets model response for a single prompt.\u001b[39;00m\n\u001b[0;32m   1361\u001b[0m \n\u001b[0;32m   1362\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;124;03m    A `MultiCandidateTextGenerationResponse` object that contains the text produced by the model.\u001b[39;00m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1401\u001b[0m prediction_request \u001b[38;5;241m=\u001b[39m _create_text_generation_prediction_request(\n\u001b[0;32m   1402\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   1403\u001b[0m     max_output_tokens\u001b[38;5;241m=\u001b[39mmax_output_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1414\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m   1415\u001b[0m )\n\u001b[1;32m-> 1417\u001b[0m prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprediction_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse_text_generation_model_multi_candidate_response(\n\u001b[0;32m   1423\u001b[0m     prediction_response\n\u001b[0;32m   1424\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\cloud\\aiplatform\\models.py:2293\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[1;34m(self, instances, parameters, timeout, use_raw_predict, use_dedicated_endpoint)\u001b[0m\n\u001b[0;32m   2284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[0;32m   2285\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   2286\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2289\u001b[0m         model_version_id\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelVersionId\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   2290\u001b[0m     )\n\u001b[0;32m   2292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2293\u001b[0m     prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2295\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2298\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata:\n\u001b[0;32m   2300\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m json_format\u001b[38;5;241m.\u001b[39mMessageToDict(prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\cloud\\aiplatform_v1\\services\\prediction_service\\client.py:887\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[1;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    886\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 887\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\anaconda\\envs\\pytorch\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\pytorch\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mFailedPrecondition\u001b[0m: 400 Project `122385713024` is not allowed to use Publisher Model `projects/elevated-module-440602-p8/locations/us-central1/publishers/google/models/text-bison@001`"
     ]
    }
   ],
   "source": [
    "generation_model.predict('what is AI?').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "def list_models(project_id, location=\"us-central1\"):\n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "    models = aiplatform.Model.list()\n",
    "    for model in models:\n",
    "        print(model.display_name, model.resource_name)\n",
    "\n",
    "list_models( \"elevated-module-440602-p8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
